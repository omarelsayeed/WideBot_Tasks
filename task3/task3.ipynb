{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install datasets\n!pip install evaluate\n!pip install accelerate -U\n!pip install transformers[torch]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModel , AutoTokenizer\nfrom transformers import AutoModelForSequenceClassification\n\ntokenizer = AutoTokenizer.from_pretrained(\"aubmindlab/bert-base-arabertv02\")\nmodel = (AutoModelForSequenceClassification.from_pretrained(\"aubmindlab/bert-base-arabertv02\", num_labels=11).to(\"cuda\"))","metadata":{"execution":{"iopub.status.busy":"2023-07-25T16:03:29.714452Z","iopub.execute_input":"2023-07-25T16:03:29.715230Z","iopub.status.idle":"2023-07-25T16:03:36.526689Z","shell.execute_reply.started":"2023-07-25T16:03:29.715140Z","shell.execute_reply":"2023-07-25T16:03:36.525587Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at aubmindlab/bert-base-arabertv02 were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at aubmindlab/bert-base-arabertv02 and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport nltk\nfrom pyarabic.araby import strip_tashkeel, strip_harakat, strip_lastharaka, strip_tatweel, normalize_hamza\nimport re\n\ndef delete_links(input_text):\n    pettern  = r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))'''\n    out_text = re.sub(pettern, ' ', input_text)\n    return out_text\n\ndef delete_repeated_characters(input_text):\n    pattern  = r'(.)\\1{2,}'\n    out_text = re.sub(pattern, r\"\\1\\1\", input_text)\n    return out_text\n\ndef replace_letters(input_text):\n    replace = {\"أ\": \"ا\",\"ة\": \"ه\",\"إ\": \"ا\",\"آ\": \"ا\",\"\": \"\"}\n    replace = dict((re.escape(k), v) for k, v in replace.items())\n    pattern = re.compile(\"|\".join(replace.keys()))\n    out_text = pattern.sub(lambda m: replace[re.escape(m.group(0))], input_text)\n    return out_text\n\ndef clean_text(input_text):\n    replace = r'[/(){}\\[\\]|@âÂ,;\\?\\'\\\"\\*…؟–’،!&\\+-:؛-]'\n    out_text = re.sub(replace, \" \", input_text)\n    words = nltk.word_tokenize(out_text)\n    out_text = ' '.join(words)\n    return out_text\n\ndef remove_vowelization(input_text):\n    vowelization = re.compile(\"\"\" ّ|َ|ً|ُ|ٌ|ِ|ٍ|ْ|ـ\"\"\", re.VERBOSE)\n    out_text = re.sub(vowelization, '', input_text)\n    return out_text\n\ndef delete_stopwords(input_text):\n    stop_words = set(nltk.corpus.stopwords.words(\"arabic\") + ['خلال' , 'الى' , 'ان' , 'او' , 'انه'])\n    tokenizer = nltk.tokenize.WhitespaceTokenizer()\n    tokens = tokenizer.tokenize(input_text)\n    out_text = [w for w in tokens if not w in stop_words]\n    out_text = ' '.join(out_text)\n    return out_text\n\n\n# improved the rouge L\ndef preprocess_text(text):\n    text = delete_links(text)\n    text = delete_repeated_characters(text)\n    text = strip_tashkeel(text)\n    text = strip_tatweel(text)\n    text= clean_text(text) \n    text = remove_vowelization(text)\n    text = replace_letters(text)\n    text = delete_stopwords(text)\n    return text\n","metadata":{"execution":{"iopub.status.busy":"2023-07-25T16:03:36.536669Z","iopub.execute_input":"2023-07-25T16:03:36.536966Z","iopub.status.idle":"2023-07-25T16:03:37.141944Z","shell.execute_reply.started":"2023-07-25T16:03:36.536938Z","shell.execute_reply":"2023-07-25T16:03:37.140893Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import glob\nimport pandas as pd\n\n\nfile_pattern = f\"/kaggle/input/hespress/stories*.csv\"\ncsv_files = glob.glob(file_pattern)\n\ndataframes = []\n\nfor file in csv_files:\n    df = pd.read_csv(file )\n    dataframes.append(df)\n    \n\nfor df in dataframes:\n    df.story = df.story.apply(preprocess_text)\n    \nfor i , df in enumerate(dataframes):\n    df.drop(['Unnamed: 0' , 'id' , 'title' , 'date' , 'author' ] , axis = 1 , inplace=  True)\n    df['topic'] = i\n    \ntrain_sets = []\ntest_sets = []\n\nfor df in dataframes:\n    train_sets.append(df[:800])\n    test_sets.append(df[800:])","metadata":{"execution":{"iopub.status.busy":"2023-07-25T16:03:37.143238Z","iopub.execute_input":"2023-07-25T16:03:37.144838Z","iopub.status.idle":"2023-07-25T16:04:57.930170Z","shell.execute_reply.started":"2023-07-25T16:03:37.144798Z","shell.execute_reply":"2023-07-25T16:04:57.929103Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"train_set = pd.concat(train_sets)\ntest_set = pd.concat(test_sets)","metadata":{"execution":{"iopub.status.busy":"2023-07-25T16:04:57.932669Z","iopub.execute_input":"2023-07-25T16:04:57.933296Z","iopub.status.idle":"2023-07-25T16:04:57.946974Z","shell.execute_reply.started":"2023-07-25T16:04:57.933257Z","shell.execute_reply":"2023-07-25T16:04:57.945859Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"file_pattern = f\"/kaggle/input/hespress/stories*.csv\"\ncsv_files = glob.glob(file_pattern)\n\nlabels = {}\nfor i,file in enumerate(csv_files):\n    labels[file.split('/')[-1][:-4]] = i","metadata":{"execution":{"iopub.status.busy":"2023-07-25T16:04:57.948476Z","iopub.execute_input":"2023-07-25T16:04:57.948943Z","iopub.status.idle":"2023-07-25T16:04:57.957527Z","shell.execute_reply.started":"2023-07-25T16:04:57.948902Z","shell.execute_reply":"2023-07-25T16:04:57.956527Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from datasets import Dataset\ntrain_set = Dataset.from_pandas(train_set)\ntest_set = Dataset.from_pandas(test_set)","metadata":{"execution":{"iopub.status.busy":"2023-07-25T16:04:57.959132Z","iopub.execute_input":"2023-07-25T16:04:57.959564Z","iopub.status.idle":"2023-07-25T16:04:58.320408Z","shell.execute_reply.started":"2023-07-25T16:04:57.959519Z","shell.execute_reply":"2023-07-25T16:04:58.319351Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import torch\nclass AraBertDataset(torch.utils.data.Dataset):\n   def __init__(self, texts, labels=None):\n       self.encodings = tokenizer(texts, padding=True, truncation=True, max_length=512)\n       self.labels = labels\n    \n   def __getitem__(self, idx):\n       item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n       if self.labels:\n           item[\"labels\"] = torch.tensor(self.labels[idx])\n       return item\n\n   def __len__(self):\n       return len(self.encodings[\"input_ids\"])","metadata":{"execution":{"iopub.status.busy":"2023-07-25T16:43:56.414388Z","iopub.execute_input":"2023-07-25T16:43:56.414789Z","iopub.status.idle":"2023-07-25T16:43:56.425757Z","shell.execute_reply.started":"2023-07-25T16:43:56.414755Z","shell.execute_reply":"2023-07-25T16:43:56.424274Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"train_dataset = AraBertDataset(train_set[\"story\"], train_set['topic'])\ntest_dataset = AraBertDataset(test_set[\"story\"], test_set['topic'])","metadata":{"execution":{"iopub.status.busy":"2023-07-25T16:06:29.783038Z","iopub.execute_input":"2023-07-25T16:06:29.783783Z","iopub.status.idle":"2023-07-25T16:06:43.185595Z","shell.execute_reply.started":"2023-07-25T16:06:29.783744Z","shell.execute_reply":"2023-07-25T16:06:43.184444Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\nimport numpy as np\ndef compute_metrics(p):\n   pred, labels = p\n   pred = np.argmax(pred, axis=1)\n   accuracy = accuracy_score(y_true=labels, y_pred=pred)\n   recall = recall_score(y_true=labels, y_pred=pred, pos_label='positive', average='weighted')\n   precision = precision_score(y_true=labels, y_pred=pred, pos_label='positive', average='weighted')\n   f1 = f1_score(y_true=labels, y_pred=pred, pos_label='positive', average='weighted')\n   return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}","metadata":{"execution":{"iopub.status.busy":"2023-07-25T16:30:00.710078Z","iopub.execute_input":"2023-07-25T16:30:00.710475Z","iopub.status.idle":"2023-07-25T16:30:00.721975Z","shell.execute_reply.started":"2023-07-25T16:30:00.710441Z","shell.execute_reply":"2023-07-25T16:30:00.720887Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments\n\nbatch_size = 16\n\nargs = TrainingArguments(\n  output_dir=\"output\",\n  evaluation_strategy=\"steps\",\n  eval_steps=200,\n  per_device_train_batch_size=batch_size,\n  per_device_eval_batch_size=batch_size,\n  num_train_epochs=1,)","metadata":{"execution":{"iopub.status.busy":"2023-07-25T16:31:13.124466Z","iopub.execute_input":"2023-07-25T16:31:13.125206Z","iopub.status.idle":"2023-07-25T16:31:13.137727Z","shell.execute_reply.started":"2023-07-25T16:31:13.125170Z","shell.execute_reply":"2023-07-25T16:31:13.136698Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stderr","text":"PyTorch: setting up devices\nThe default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import Trainer\ntrainer = Trainer(\n  model=model,\n  args=args,\n  train_dataset=train_dataset,\n  eval_dataset=test_dataset,\n  compute_metrics=compute_metrics\n)","metadata":{"execution":{"iopub.status.busy":"2023-07-25T16:31:13.739099Z","iopub.execute_input":"2023-07-25T16:31:13.739483Z","iopub.status.idle":"2023-07-25T16:31:13.758913Z","shell.execute_reply.started":"2023-07-25T16:31:13.739448Z","shell.execute_reply":"2023-07-25T16:31:13.757313Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"trainer.train()\n# res = trainer.evaluate()","metadata":{"execution":{"iopub.status.busy":"2023-07-25T16:31:13.994535Z","iopub.execute_input":"2023-07-25T16:31:13.995659Z","iopub.status.idle":"2023-07-25T16:40:17.914765Z","shell.execute_reply.started":"2023-07-25T16:31:13.995612Z","shell.execute_reply":"2023-07-25T16:40:17.913645Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 8800\n  Num Epochs = 1\n  Instantaneous batch size per device = 16\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 1\n  Total optimization steps = 550\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='550' max='550' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [550/550 09:02, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>200</td>\n      <td>No log</td>\n      <td>0.642451</td>\n      <td>0.855000</td>\n      <td>0.856851</td>\n      <td>0.855000</td>\n      <td>0.853789</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>No log</td>\n      <td>0.512293</td>\n      <td>0.861364</td>\n      <td>0.860911</td>\n      <td>0.861364</td>\n      <td>0.859744</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"***** Running Evaluation *****\n  Num examples = 2200\n  Batch size = 16\n/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1375: UserWarning: Note that pos_label (set to 'positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.\n  UserWarning,\n/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1375: UserWarning: Note that pos_label (set to 'positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.\n  UserWarning,\n/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1375: UserWarning: Note that pos_label (set to 'positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.\n  UserWarning,\n***** Running Evaluation *****\n  Num examples = 2200\n  Batch size = 16\n/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1375: UserWarning: Note that pos_label (set to 'positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.\n  UserWarning,\n/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1375: UserWarning: Note that pos_label (set to 'positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.\n  UserWarning,\n/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1375: UserWarning: Note that pos_label (set to 'positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.\n  UserWarning,\nSaving model checkpoint to output/checkpoint-500\nConfiguration saved in output/checkpoint-500/config.json\nModel weights saved in output/checkpoint-500/pytorch_model.bin\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n","output_type":"stream"},{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=550, training_loss=0.28114410053599964, metrics={'train_runtime': 543.8838, 'train_samples_per_second': 16.18, 'train_steps_per_second': 1.011, 'total_flos': 2315564386713600.0, 'train_loss': 0.28114410053599964, 'epoch': 1.0})"},"metadata":{}}]},{"cell_type":"code","source":"topics = []\nfor df in test_sets:\n    topic = df.topic[800]\n    df = Dataset.from_pandas(df)\n    test_df = AraBertDataset(df[\"story\"], df['topic'])\n    trainer = Trainer(\n      model=model,\n      args=args,\n      train_dataset=train_dataset,\n      eval_dataset=test_df,\n      compute_metrics=compute_metrics\n    )\n    topics.append((trainer.evaluate() , topic))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"topics","metadata":{"execution":{"iopub.status.busy":"2023-07-25T16:46:34.727230Z","iopub.execute_input":"2023-07-25T16:46:34.727663Z","iopub.status.idle":"2023-07-25T16:46:34.741720Z","shell.execute_reply.started":"2023-07-25T16:46:34.727626Z","shell.execute_reply":"2023-07-25T16:46:34.740465Z"},"trusted":true},"execution_count":46,"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"[({'eval_loss': 0.11061007529497147,\n   'eval_accuracy': 0.97,\n   'eval_precision': 1.0,\n   'eval_recall': 0.97,\n   'eval_f1': 0.9847715736040609,\n   'eval_runtime': 3.4874,\n   'eval_samples_per_second': 57.349,\n   'eval_steps_per_second': 3.728},\n  0),\n ({'eval_loss': 0.11591026932001114,\n   'eval_accuracy': 0.965,\n   'eval_precision': 1.0,\n   'eval_recall': 0.965,\n   'eval_f1': 0.9821882951653944,\n   'eval_runtime': 3.4314,\n   'eval_samples_per_second': 58.285,\n   'eval_steps_per_second': 3.789},\n  1),\n ({'eval_loss': 0.47806477546691895,\n   'eval_accuracy': 0.865,\n   'eval_precision': 1.0,\n   'eval_recall': 0.865,\n   'eval_f1': 0.9276139410187667,\n   'eval_runtime': 3.4462,\n   'eval_samples_per_second': 58.034,\n   'eval_steps_per_second': 3.772},\n  2),\n ({'eval_loss': 0.10588047653436661,\n   'eval_accuracy': 0.985,\n   'eval_precision': 1.0,\n   'eval_recall': 0.985,\n   'eval_f1': 0.9924433249370278,\n   'eval_runtime': 3.4462,\n   'eval_samples_per_second': 58.035,\n   'eval_steps_per_second': 3.772},\n  3),\n ({'eval_loss': 0.2898067235946655,\n   'eval_accuracy': 0.93,\n   'eval_precision': 1.0,\n   'eval_recall': 0.93,\n   'eval_f1': 0.9637305699481865,\n   'eval_runtime': 3.4539,\n   'eval_samples_per_second': 57.905,\n   'eval_steps_per_second': 3.764},\n  4),\n ({'eval_loss': 0.52960604429245,\n   'eval_accuracy': 0.85,\n   'eval_precision': 1.0,\n   'eval_recall': 0.85,\n   'eval_f1': 0.9189189189189189,\n   'eval_runtime': 3.432,\n   'eval_samples_per_second': 58.276,\n   'eval_steps_per_second': 3.788},\n  5),\n ({'eval_loss': 0.060862768441438675,\n   'eval_accuracy': 0.985,\n   'eval_precision': 1.0,\n   'eval_recall': 0.985,\n   'eval_f1': 0.9924433249370278,\n   'eval_runtime': 3.4281,\n   'eval_samples_per_second': 58.341,\n   'eval_steps_per_second': 3.792},\n  6),\n ({'eval_loss': 1.3029022216796875,\n   'eval_accuracy': 0.63,\n   'eval_precision': 1.0,\n   'eval_recall': 0.63,\n   'eval_f1': 0.7730061349693252,\n   'eval_runtime': 3.4368,\n   'eval_samples_per_second': 58.193,\n   'eval_steps_per_second': 3.783},\n  7),\n ({'eval_loss': 1.5952624082565308,\n   'eval_accuracy': 0.565,\n   'eval_precision': 1.0,\n   'eval_recall': 0.565,\n   'eval_f1': 0.7220447284345047,\n   'eval_runtime': 3.4432,\n   'eval_samples_per_second': 58.085,\n   'eval_steps_per_second': 3.776},\n  8),\n ({'eval_loss': 0.5739114880561829,\n   'eval_accuracy': 0.825,\n   'eval_precision': 1.0,\n   'eval_recall': 0.825,\n   'eval_f1': 0.9041095890410957,\n   'eval_runtime': 3.4308,\n   'eval_samples_per_second': 58.295,\n   'eval_steps_per_second': 3.789},\n  9),\n ({'eval_loss': 0.39755675196647644,\n   'eval_accuracy': 0.895,\n   'eval_precision': 1.0,\n   'eval_recall': 0.895,\n   'eval_f1': 0.9445910290237467,\n   'eval_runtime': 3.4401,\n   'eval_samples_per_second': 58.139,\n   'eval_steps_per_second': 3.779},\n  10)]"},"metadata":{}}]},{"cell_type":"code","source":"topics","metadata":{"execution":{"iopub.status.busy":"2023-07-25T16:45:20.486264Z","iopub.execute_input":"2023-07-25T16:45:20.486707Z","iopub.status.idle":"2023-07-25T16:45:20.504824Z","shell.execute_reply.started":"2023-07-25T16:45:20.486668Z","shell.execute_reply":"2023-07-25T16:45:20.502558Z"},"trusted":true},"execution_count":44,"outputs":[{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"[({'eval_loss': 0.11061007529497147,\n   'eval_accuracy': 0.97,\n   'eval_precision': 1.0,\n   'eval_recall': 0.97,\n   'eval_f1': 0.9847715736040609,\n   'eval_runtime': 3.4874,\n   'eval_samples_per_second': 57.349,\n   'eval_steps_per_second': 3.728},\n  0),\n ({'eval_loss': 0.11591026932001114,\n   'eval_accuracy': 0.965,\n   'eval_precision': 1.0,\n   'eval_recall': 0.965,\n   'eval_f1': 0.9821882951653944,\n   'eval_runtime': 3.4314,\n   'eval_samples_per_second': 58.285,\n   'eval_steps_per_second': 3.789},\n  1),\n ({'eval_loss': 0.47806477546691895,\n   'eval_accuracy': 0.865,\n   'eval_precision': 1.0,\n   'eval_recall': 0.865,\n   'eval_f1': 0.9276139410187667,\n   'eval_runtime': 3.4462,\n   'eval_samples_per_second': 58.034,\n   'eval_steps_per_second': 3.772},\n  2),\n ({'eval_loss': 0.10588047653436661,\n   'eval_accuracy': 0.985,\n   'eval_precision': 1.0,\n   'eval_recall': 0.985,\n   'eval_f1': 0.9924433249370278,\n   'eval_runtime': 3.4462,\n   'eval_samples_per_second': 58.035,\n   'eval_steps_per_second': 3.772},\n  3),\n ({'eval_loss': 0.2898067235946655,\n   'eval_accuracy': 0.93,\n   'eval_precision': 1.0,\n   'eval_recall': 0.93,\n   'eval_f1': 0.9637305699481865,\n   'eval_runtime': 3.4539,\n   'eval_samples_per_second': 57.905,\n   'eval_steps_per_second': 3.764},\n  4),\n ({'eval_loss': 0.52960604429245,\n   'eval_accuracy': 0.85,\n   'eval_precision': 1.0,\n   'eval_recall': 0.85,\n   'eval_f1': 0.9189189189189189,\n   'eval_runtime': 3.432,\n   'eval_samples_per_second': 58.276,\n   'eval_steps_per_second': 3.788},\n  5),\n ({'eval_loss': 0.060862768441438675,\n   'eval_accuracy': 0.985,\n   'eval_precision': 1.0,\n   'eval_recall': 0.985,\n   'eval_f1': 0.9924433249370278,\n   'eval_runtime': 3.4281,\n   'eval_samples_per_second': 58.341,\n   'eval_steps_per_second': 3.792},\n  6),\n ({'eval_loss': 1.3029022216796875,\n   'eval_accuracy': 0.63,\n   'eval_precision': 1.0,\n   'eval_recall': 0.63,\n   'eval_f1': 0.7730061349693252,\n   'eval_runtime': 3.4368,\n   'eval_samples_per_second': 58.193,\n   'eval_steps_per_second': 3.783},\n  7),\n ({'eval_loss': 1.5952624082565308,\n   'eval_accuracy': 0.565,\n   'eval_precision': 1.0,\n   'eval_recall': 0.565,\n   'eval_f1': 0.7220447284345047,\n   'eval_runtime': 3.4432,\n   'eval_samples_per_second': 58.085,\n   'eval_steps_per_second': 3.776},\n  8),\n ({'eval_loss': 0.5739114880561829,\n   'eval_accuracy': 0.825,\n   'eval_precision': 1.0,\n   'eval_recall': 0.825,\n   'eval_f1': 0.9041095890410957,\n   'eval_runtime': 3.4308,\n   'eval_samples_per_second': 58.295,\n   'eval_steps_per_second': 3.789},\n  9),\n ({'eval_loss': 0.39755675196647644,\n   'eval_accuracy': 0.895,\n   'eval_precision': 1.0,\n   'eval_recall': 0.895,\n   'eval_f1': 0.9445910290237467,\n   'eval_runtime': 3.4401,\n   'eval_samples_per_second': 58.139,\n   'eval_steps_per_second': 3.779},\n  10)]"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}