{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from pyarabic.araby import strip_tashkeel, strip_harakat, strip_lastharaka, strip_tatweel, normalize_hamza\n",
    "import re\n",
    "\n",
    "def delete_links(input_text):\n",
    "    pettern  = r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))'''\n",
    "    out_text = re.sub(pettern, ' ', input_text)\n",
    "    return out_text\n",
    "\n",
    "def delete_repeated_characters(input_text):\n",
    "    pattern  = r'(.)\\1{2,}'\n",
    "    out_text = re.sub(pattern, r\"\\1\\1\", input_text)\n",
    "    return out_text\n",
    "\n",
    "def replace_letters(input_text):\n",
    "    replace = {\"أ\": \"ا\",\"ة\": \"ه\",\"إ\": \"ا\",\"آ\": \"ا\",\"\": \"\"}\n",
    "    replace = dict((re.escape(k), v) for k, v in replace.items())\n",
    "    pattern = re.compile(\"|\".join(replace.keys()))\n",
    "    out_text = pattern.sub(lambda m: replace[re.escape(m.group(0))], input_text)\n",
    "    return out_text\n",
    "\n",
    "def clean_text(input_text):\n",
    "    replace = r'[/(){}\\[\\]|@âÂ,;\\?\\'\\\"\\*…؟–’،!&\\+-:؛-]'\n",
    "    out_text = re.sub(replace, \" \", input_text)\n",
    "    words = nltk.word_tokenize(out_text)\n",
    "    out_text = ' '.join(words)\n",
    "    return out_text\n",
    "\n",
    "def remove_vowelization(input_text):\n",
    "    vowelization = re.compile(\"\"\" ّ|َ|ً|ُ|ٌ|ِ|ٍ|ْ|ـ\"\"\", re.VERBOSE)\n",
    "    out_text = re.sub(vowelization, '', input_text)\n",
    "    return out_text\n",
    "\n",
    "def delete_stopwords(input_text):\n",
    "    stop_words = set(nltk.corpus.stopwords.words(\"arabic\") + ['خلال' , 'الى' , 'ان' , 'او' , 'انه'])\n",
    "    tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "    tokens = tokenizer.tokenize(input_text)\n",
    "    out_text = [w for w in tokens if not w in stop_words]\n",
    "    out_text = ' '.join(out_text)\n",
    "    return out_text\n",
    "\n",
    "\n",
    "# improved the rouge L\n",
    "def preprocess_text(text):\n",
    "    text = delete_links(text)\n",
    "    text = delete_repeated_characters(text)\n",
    "    text = strip_tashkeel(text)\n",
    "    text = strip_tatweel(text)\n",
    "    text= clean_text(text) \n",
    "    text = remove_vowelization(text)\n",
    "    text = replace_letters(text)\n",
    "    text = delete_stopwords(text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "file_pattern = f\"stories*.csv\"\n",
    "csv_files = glob.glob(file_pattern)\n",
    "\n",
    "dataframes = []\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(file )\n",
    "    dataframes.append(df)\n",
    "\n",
    "for df in dataframes:\n",
    "    df.story = df.story.apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_word_count(df):\n",
    "    return df['story'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Create a new DataFrame to store word count for each DataFrame\n",
    "word_count_df = pd.DataFrame()\n",
    "\n",
    "# Iterate through each DataFrame in the list\n",
    "for idx, df in enumerate(dataframes, 1):\n",
    "    # Calculate word count for each row in the current DataFrame\n",
    "    word_count_col = f\"{str(df.topic[0])}\"\n",
    "    word_count_df[word_count_col] = calculate_word_count(df)\n",
    "\n",
    "# Output the final DataFrame with word count for each DataFrame\n",
    "word_count_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming you have already loaded your DataFrame\n",
    "# word_count_df = pd.DataFrame({...})\n",
    "\n",
    "# Calculate the average of each column\n",
    "column_averages = word_count_df.mean()\n",
    "\n",
    "# Define the number of columns in your DataFrame\n",
    "num_columns = len(word_count_df.columns)\n",
    "\n",
    "# Calculate the number of rows and columns for subplots\n",
    "num_rows = (num_columns + 2) // 3\n",
    "num_cols = min(3, num_columns)\n",
    "\n",
    "# Create subplots\n",
    "fig, axs = plt.subplots(num_rows, num_cols, figsize=(15, 5*num_rows))\n",
    "\n",
    "# Flatten the axs array in case we have fewer than 3 columns\n",
    "axs = axs.flatten()\n",
    "\n",
    "# Plot histograms and add average annotations for each column\n",
    "for i, column in enumerate(word_count_df.columns):\n",
    "    sns.histplot(word_count_df[column], ax=axs[i], kde=True)\n",
    "    axs[i].set_title(f\"Histogram of {column}\")\n",
    "    axs[i].set_xlabel(column)\n",
    "\n",
    "    # Calculate the average value for the current column\n",
    "    avg_value = column_averages[column]\n",
    "    axs[i].axvline(avg_value, color='red', linestyle='dashed', linewidth=2)\n",
    "    axs[i].text(avg_value, axs[i].get_ylim()[1]*0.9, f'Average: {avg_value:.2f}', color='red', ha='center')\n",
    "\n",
    "# Remove any empty subplots\n",
    "for i in range(num_columns, num_rows * num_cols):\n",
    "    axs[i].axis('off')\n",
    "\n",
    "# Adjust layout to avoid overlapping titles\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('المغرب', 11447),\n",
       " ('المغربيه', 10404),\n",
       " ('كورونا', 7743),\n",
       " ('الامازيغيه', 7561),\n",
       " ('المغربي', 6546),\n",
       " ('حاله', 6390),\n",
       " ('اجل', 6147),\n",
       " ('سنه', 5964),\n",
       " ('وفي', 5775),\n",
       " ('الحكومه', 5667)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_dataframes = pd.concat(dataframes)\n",
    "# get the most common words\n",
    "from collections import Counter\n",
    "all_words = ' '.join(all_dataframes['story']).split()\n",
    "word_count = Counter(all_words)\n",
    "word_count.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('فيروس', 'كورونا'), 2840),\n",
       " (('كورونا', 'المستجد'), 1949),\n",
       " (('الحجر', 'الصحي'), 1624),\n",
       " (('محمد', 'السادس'), 1602),\n",
       " (('الدار', 'البيضاء'), 1451),\n",
       " (('النيابه', 'العامه'), 1286),\n",
       " (('جائحه', 'كورونا'), 1259),\n",
       " (('بفيروس', 'كورونا'), 1233),\n",
       " (('رئيس', 'الحكومه'), 1232),\n",
       " (('الملك', 'محمد'), 1209)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get most frequent 2-grams\n",
    "from nltk import ngrams\n",
    "n = 2\n",
    "ngrams_freq = Counter(ngrams(all_words, n))\n",
    "ngrams_freq.most_common(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('الملك', 'محمد', 'السادس'), 1179),\n",
       " (('فيروس', 'كورونا', 'المستجد'), 1112),\n",
       " (('لجريده', 'هسبريس', 'الالكترونيه'), 827),\n",
       " (('تصريح', 'لجريده', 'هسبريس'), 680),\n",
       " (('بفيروس', 'كورونا', 'المستجد'), 652),\n",
       " (('حاله', 'الطوارئ', 'الصحيه'), 630),\n",
       " (('النيابه', 'العامه', 'المختصه'), 616),\n",
       " (('سعد', 'الدين', 'العثماني'), 596),\n",
       " (('الملكي', 'للثقافه', 'الامازيغيه'), 527),\n",
       " (('حزب', 'العداله', 'والتنميه'), 477)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get most frequent 2-grams\n",
    "from nltk import ngrams\n",
    "n = 3\n",
    "ngrams_freq = Counter(ngrams(all_words, n))\n",
    "ngrams_freq.most_common(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
